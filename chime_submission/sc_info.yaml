team_name: BUT/JHU
affiliation: Brno University of Technology, Johns Hopkins University
# unique name for this submission, please keep it the same in the technical description paper
# so it helps reviewers for jury mention for practicality and efficiency.
system_tag: System1_SC
cmt_paper_id: 12 # cmt paper id
contact_email: ipoloka@fit.vutbr.cz, ihan@fit.vut.cz, xkleme15@stud.fit.vutbr.cz, wiesner@jhu.edu

## NOTE, in the following if you want to put multiple answers please,
## separate with commas e.g. wavlm,wav2vec2,hubert
## If the attribute does not apply e.g. you don't have SSE (speech separation and enhancement), leave it blank.
## In doubt, if you are unsure about a field, leave it blank.

# ranking score as obtained on development set with the leaderboard
# we use this to doublecheck that this submission is okay.
# if there are some errors etc. we will reach out.
ranking_score:
  macro:
  chime6:
  dipco:
  mixer6:
  notsofar1:

# all figures here on the final evaluation set
inference:
  approx_tot_time: 0.83 * (1) + 0.4 * (10) # numbers in brackets correspond to number of GPU machines used
  num_gpus: 10 # number of GPUs, if multiple nodes sum the total across all nodes
  gpu_type: NVIDIA RTX A5000 # type of GPUs used if multiple types use commas h100,a100
  num_cpus: 40 # number of CPUs, if multiple nodes sum the total across all nodes
  cpus_type: Intel(R) Xeon(R) CPU E5-2640 v4 # type of CPUs used if multiple type use commas epyc7F72,epyc7F52

# component level analysis
asr:
  external_models_used: Whisper-large-v3 # e.g. wavlm,hubert
  num_ensembled_sys: 1 # 1 if no ensemble
  tot_parameters: 1,624,680,960 # sum across all ensembled systems
  lm:
    type: nan # e.g. neural, n-gram statistical
    external_models_used: 0 # e.g. LLM model
    tot_parameters: nan
  training: # also for this, sum across all ensembled systems
    approx_tot_time: 20 # in hours e.g. 68 h report the sum here
    external_data_used: AMI, LibriSpeech # e.g. AMI,LibriSpeech
    tot_hours_pre_augmentation: 1065 # tot hours of data before augmentation
  inference:
      approx_tot_time: 0.4 # in hours e.g. 28 hours etc for inference on all evaluation
      num_gpus: 10 # number of GPUs, if multiple nodes sum the total across all nodes
      gpu_type: NVIDIA RTX A5000 # type of GPUs used if multiple types use commas h100,a100
      num_cpus: 40 # number of CPUs, if multiple nodes sum the total across all nodes
      cpus_type: Intel(R) Xeon(R) CPU E5-2640 v4 # type of CPUs used if multiple type use commas epyc7F72,epyc7F52

diarization:
  external_models_used: wavlm-base-plus # e.g. wavlm,hubert
  num_ensembled_sys: 1 # 1 if no ensemble
  tot_parameters: 100,673,928 # sum across all ensembled systems
  training: # also for this, sum across all ensembled systems
    approx_tot_time: 18 # in hours e.g. 27 h, report the sum here
    external_data_used: AMI # e.g. AMI,LibriSpeech
    tot_hours_pre_augmentation: 374 # tot hours of data before augmentation
  inference:
      approx_tot_time: 0.83 # in hours e.g. 28 hours etc for inference on all evaluation
      num_gpus: 1 # number of GPUs, if multiple nodes sum the total across all nodes
      gpu_type: NVIDIA RTX A5000 # type of GPUs used if multiple types use commas h100,a100
      num_cpus: 1 # number of CPUs, if multiple nodes sum the total across all nodes
      cpus_type: Intel(R) Xeon(R) CPU E5-2640 v4 # type of CPUs used if multiple type use commas epyc7F72,epyc7F52

sse_frontend: # speech separation and enhancement frontend
  external_models_used: # e.g. wavlm,hubert
  num_ensembled_sys: # 1 if no ensemble
  tot_parameters: # sum across all ensembled systems
  training: # also for this, sum across all ensembled systems
    approx_tot_time: # in hours e.g. 68 h
    external_data_used: # e.g. AMI,LibriSpeech
    tot_hours_pre_augmentation: # tot hours of data before augmentation
  inference:
    approx_tot_time: # in hours e.g. 28 hours etc for inference on all evaluation
      num_gpus: # number of GPUs, if multiple nodes sum the total across all nodes
      gpu_type: # type of GPUs used if multiple types use commas h100,a100
      num_cpus: # number of CPUs, if multiple nodes sum the total across all nodes
      cpus_type: # type of CPUs used if multiple type use commas epyc7F72,epyc7F52
