# @package _global_
defaults:
  - /train/icassp/mt_asr/base

experiment: nsf_turbo_v5
model:
  whisper_model: openai/whisper-large-v3-turbo
  reinit_encoder_from: ${oc.env:PRETRAINED_CTC_MODELS_PATH}/${model.whisper_model}_ctc-pretrain_libri/model.safetensors
  mt_num_speakers: 8
  params_to_keep_frozen_keywords:
  - "decoder"
  prefixes_to_preheat: [ 'model.encoder.additional_layer', 'model.encoder.additional_self_attention_layer', 'model.encoder.lm_head', 'model.encoder.subsample_conv1', 'model.encoder.subsample_conv2', 'model.encoder.target_amplifiers', 'model.encoder.blank_projection', 'model.encoder.modifiers','model.encoder.target_embeddings_proj', 'model.encoder.interaction' ]


data:
  train_text_norm: "whisper"
  use_timestamps: true
  train_cutsets:
  - ${oc.env:MANIFEST_DIR}/notsofar_train_sc_cutset_30s.jsonl.gz
  dev_cutsets: ${oc.env:MANIFEST_DIR}/notsofar_dev_sc_cutset.jsonl.gz
  eval_cutsets: ${oc.env:MANIFEST_DIR}/notsofar_eval_sc_cutset.jsonl.gz

training:
  remove_timestamps_from_ctc: true
  auto_find_batch_size: false
  overall_batch_size: 8
  per_device_train_batch_size: 1 #  It's set to overall_batch_size // (num_gpus * gradient_accumulation_steps)
  gradient_accumulation_steps: 2
  dataloader_num_workers: 8