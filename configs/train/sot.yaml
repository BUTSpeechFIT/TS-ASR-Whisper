# @package _global_
experiment: sot_ami_iterative_decoding_v3
model:
  whisper_model: openai/whisper-large-v3-turbo
  reinit_encoder_from: null
  params_to_keep_frozen_keywords: []
  ctc_weight: 0.0

data:
  use_timestamps: false
  train_cutsets:
  - ${oc.env:MANIFEST_DIR}/ami/ami-sdm_cutset_train_30s.jsonl.gz
  dev_cutsets:
  - ${oc.env:MANIFEST_DIR}/ami/ami-sdm_cutset_dev_utt_group_max_30s.jsonl.gz
  eval_cutsets:
    - /mnt/scratch/tmp/ipoloka/mt_asr_data/manifests/ami/ami-sdm_cutset_test_utt_group_kaldi_30m.jsonl.gz
  dataset_weights:
    - 1

aug:
  musan_root: ${oc.env:MUSAN_ROOT}
  musan_augment_prob: 0.3

training:
  train_sot: true
  remove_timestamps_from_ctc: true
  eval_strategy: steps
  save_strategy: steps
  eval_steps: 500
  save_steps: 500
  overall_batch_size: 32
  generation_max_length: 440
  learning_rate: 2e-6
  per_device_eval_batch_size: 8
  dataloader_num_workers: 1
  dataloader_prefetch_factor: 2
  dataloader_pin_memory: true
  use_fddt_only_n_epochs: 0
  use_fddt_only_n_steps: 0
  use_fddt: false
  metric_for_best_model: eval_ami-sdm_cutset_dev_utt_group_max_30s_error_rate
  watch_grads: false
  max_steps: 20000
  lr_scheduler_type: cosine
  train_metrics_list: ["cp_wer"]
  eval_metrics_list: ["cp_wer"]
  early_stopping_patience: -1

wandb:
  project: "sot"