# @package _global_
experiment: sot_ami_slf
model:
  whisper_model: openai/whisper-large-v3-turbo
  reinit_encoder_from: ${oc.env:PRETRAINED_CTC_MODELS_PATH}/${model.whisper_model}_ctc-pretrain_libri/model.safetensors
  params_to_keep_frozen_keywords: []
  ctc_weight: 0

data:
  use_timestamps: false
  sot_strategy: "speaker_longest_first"
  train_cutsets:
  - ${oc.env:MANIFEST_DIR}/ami/ami-sdm_cutset_train_30s.jsonl.gz
  dev_cutsets:
  - ${oc.env:MANIFEST_DIR}/ami/ami-sdm_cutset_dev_utt_group_max_30s.jsonl.gz
  eval_cutsets:
  - ${oc.env:MANIFEST_DIR}/ami/ami-sdm_cutset_test_utt_group_kaldi_30m.jsonl.gz
  dataset_weights:
    - 1

aug:
  musan_root: ${oc.env:MUSAN_ROOT}
  musan_augment_prob: 0.3

training:
  train_sot: true
  remove_timestamps_from_ctc: true
  eval_strategy: steps
  save_strategy: steps
  eval_steps: 500
  save_steps: 500
  overall_batch_size: 64
  generation_max_length: 440
  learning_rate: 4e-6
  per_device_eval_batch_size: 8
  dataloader_num_workers: 2
  dataloader_prefetch_factor: 2
  dataloader_pin_memory: true
  use_fddt_only_n_epochs: 0
  use_fddt_only_n_steps: 0
  use_fddt: false
  metric_for_best_model: eval_ami-sdm_cutset_dev_utt_group_max_30s_error_rate
  watch_grads: false
  max_steps: 10000
  warmup_steps: 2500
  weight_decay: 0.01
  lr_scheduler_type: cosine
  train_metrics_list: ["cp_wer"]
  eval_metrics_list: ["cp_wer"]
  early_stopping_patience: -1

wandb:
  project: "sot_v2"