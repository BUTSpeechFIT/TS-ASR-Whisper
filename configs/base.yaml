experiment: DEFAULT_EXPERIMENT
exp_dir: exp

model:
  ctc_weight: 0.3
  whisper_model: "openai/whisper-tiny"
  reinit_encoder_from: null
  reinit_from: null
  fddt_is_diagonal: true
  fddt_bias_only: false
  fddt_use_silence: true
  fddt_use_target: true
  fddt_use_overlap: true
  fddt_use_non_target: true
  apply_fddt_to_n_layers: -1  # Means all layers
  non_target_fddt_value: 0.5
  fddt_init: "suppressive"
  prefixes_to_preheat: ['model.encoder.additional_layer', 'model.encoder.additional_self_attention_layer', 'model.encoder.lm_head', 'model.encoder.subsample_conv1', 'model.encoder.subsample_conv2', 'model.encoder.fddts', 'model.encoder.initial_fddt']
  pre_ctc_sub_sample: true
  additional_self_attention_layer: true

data:
  use_libri: false
  train_cutsets:
    - ${oc.env:MANIFEST_DIR}/notsofar1/notsofar1-sdm_cutset_train_sc_30s.jsonl.gz
  dev_cutsets: ${oc.env:MANIFEST_DIR}/notsofar1/notsofar1-sdm_cutset_dev_sc.jsonl.gz
  eval_cutsets: ${oc.env:MANIFEST_DIR}/notsofar1/notsofar1-sdm_cutset_eval_sc.jsonl.gz
  use_timestamps: true
  train_text_norm: "whisper_nsf"
  eval_text_norm: "whisper_nsf"
  dataset_weights: null

  global_lang_id: "en"
  provide_gt_lang: true
  load_channel_zero_only: false

aug:
  stno_gaussian_noise_var: null
  stno_gaussian_noise_prob: 0.0
  do_augment: false

decoding:
  decoding_ctc_weight: 0.0
  condition_on_prev: false
  length_penalty: 1.0

training:
  auto_find_batch_size: true
  bf16: true
  bf16_full_eval: true
  use_flash_attention: true
  dataloader_num_workers: 2
  dataloader_prefetch_factor: 1
  dataloader_pin_memory: false
  overall_batch_size: 64
  decode_only: false
  use_custom_optimizer: true
  use_fddt_only_n_epochs: 1
  remove_timestamps_from_ctc: false
  fddt_lr_multiplier: 100.0
  use_fddt: true
  per_device_train_batch_size: 1 #  It's set to overall_batch_size // (num_gpus * gradient_accumulation_steps)
  per_device_eval_batch_size: 16
  max_steps: 50000
  num_train_epochs: 10
  early_stopping_patience: 5
  gradient_accumulation_steps: 1
  learning_rate: 2e-6
  warmup_steps: 2000
  weight_decay: 0.0
  greater_is_better: false
  ddp_find_unused_parameters: false
  generation_max_length: 445
  generation_num_beams: 1
  predict_with_generate: true
  watch_grads: false
  store_src: false

  eval_strategy: "epoch"
  save_strategy: "epoch"
  eval_steps: 1000 #  If strategy is steps, it will evaluate every eval_steps
  save_steps: 1000 #  If strategy is steps, it will save every save_steps
  save_total_limit: 1

  metric_for_best_model: eval_notsofar_eval_sc_cutset_tcp_wer
  train_metrics_list: ["tcp_wer", "cp_wer"]
  eval_metrics_list: ["tcp_wer", "cp_wer"]

  do_train: true
  load_best_model_at_end: true
  logging_steps: 5
  eval_delay: 2

  output_dir: ${oc.env:EXPERIMENT_PATH}/${exp_dir}/${experiment}
  run_name: ${experiment}
  save_visualizations: false
  remove_unused_columns: false

wandb:
  project: "dicow"